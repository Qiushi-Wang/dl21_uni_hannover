{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5_RNN_QiushiWang_10033405.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "saItxTU2Vs9p"
      },
      "source": [
        "# Deep Learning 2021\n",
        "## Assignment 5 - Recurrent Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lj5q5JTHVs-b"
      },
      "source": [
        "### 1. Word Embeddings\n",
        "Consider a Word2Vec model with the vocabulary $\\{A, B, C, D, E\\}$ and the weight matrices\n",
        "\\begin{equation*}\n",
        "    W =\n",
        "        \\begin{pmatrix}\n",
        "            1 & -1  & 0\\\\\n",
        "            0 & 1   & 2\\\\\n",
        "            2 & 2   & 2\\\\\n",
        "            2 & 1   & 0\\\\\n",
        "            2 & -1  & 0\n",
        "        \\end{pmatrix}\n",
        "    \\quad \\text{and} \\quad\n",
        "    W' =\n",
        "        \\begin{pmatrix}\n",
        "            2 & 3   & 4   & 2 & 0\\\\\n",
        "            1 & 3   & -1  & 2 & 3\\\\\n",
        "            1 & -2  & 1   & 0 & 1\n",
        "        \\end{pmatrix}.\n",
        "\\end{equation*}\n",
        "Assume that the model uses the __CBOW__ architecture and that the one-hot indices correspond to the order of the words in the vocabulary above (the one-hot vector $(1, 0, 0, 0, 0)$ encodes the word $A$, the vector $(0, 1, 0, 0, 0)$ encodes $B$ and so on).\n",
        "\n",
        "1. What is one way the word $A$ can be embedded using these matrices?\n",
        "2. Suppose that at one point in time during training $W$ and $W'$ have the above values and the window is $B\\ C\\ D$. What would be the corresponding network input and output?\n",
        "3. What loss function is used in Word2Vec? What would be the inputs of the loss function in the given example?\n",
        "4. Compute the loss for the given example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqF3wY2LgxyE"
      },
      "source": [
        "#### Solution\n",
        "\n",
        "1. The one-hot vektor of word A is (1,0,0,0,0), product the vektor with W， in hidden layer we can get the a vektor of A: $h = W^T*A = W_1$, which is the first row of matrice W. Then left-product the matrice W' with the hidden layer vektor we can get another embedding vektor:$W'^T * h$\n",
        "2. input:\n",
        "$$\n",
        "(0,1,0,0,0)\\\\\n",
        "(0,0,1,0,0)\\\\\n",
        "(0,0,0,1,0)\n",
        "$$\n",
        "hiddenlayer:\n",
        "$$\n",
        "h = \\frac{1}{3} * W^T * (B + C + D)\\\\\n",
        "  = (4/3, 4/3, 4/3)\n",
        "$$\n",
        "y:\n",
        "$$\n",
        "y = W'h = (16/3, 16/3, 16/3, 16/3, 16/3)\n",
        "$$\n",
        "3. binary cross entropy loss, the input is 0 or 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlsQ-gFiVs-d"
      },
      "source": [
        "### 2. Backpropagation through Time \n",
        "What happens to the gradient in vanilla RNNs if you backpropagate through a long sequence? What are some of the possible solutions proposed in literature to solve such problems?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBBYm4lag0nx"
      },
      "source": [
        "#### Solution\n",
        "\n",
        "Your solution goes here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQR03x-fOzKe"
      },
      "source": [
        "### C0.\n",
        "We recommend reading [this tutorial](https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html) on building an LSTM model in PyTorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdYYZ8CbD7WL"
      },
      "source": [
        "### C1. Word2Vec\n",
        "The [MovieLens 25M](https://grouplens.org/datasets/movielens/25m/) dataset contains movie titles and corresponding tags added by users. For every movie in the dataset, we concatenate all tags and treat the resulting list of tags as a sentence.\n",
        "\n",
        "Your task is to build a simple search engine based on Word2Vec, treating each tag as a word:\n",
        "\n",
        "1. Train a Word2Vec model on the tag sentences to obtain $64$-dimensional word embeddings. Use the [gensim's Word2Vec](https://radimrehurek.com/gensim/models/word2vec.html) function for this task. Set the window size to $5$ and the min_count to 1.\n",
        "\n",
        "Now suppose we represent a movie $m$ that has a set of tags $T$ as the average of all word vectors, i.e.\n",
        "\\begin{equation}\n",
        "    v_m = \\frac{\\sum_{t \\in T} E(t)}{|T|}\n",
        "\\end{equation}\n",
        "where $E(t)$ is the embedding of a tag $t$ and $v_m$ is the vector representation of $m$.\n",
        "\n",
        "2. Implement a small search engine, where each movie is represented by a vector as described above. Queries are represented the same way. The relevance of a movie w.r.t. a query should be the cosine similarity between the two vectors. Print the top-$10$ results (the movie titles) for the query title `Toy Story (1995)`. Hint: the most relevant movie to a query is the query movie itself. It should have a cosine similarity of 1.0 ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpqaFRHQD7WN"
      },
      "source": [
        "Run the cells below to download, extract and setup the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHKnpNFdOzKf",
        "outputId": "e75943b7-1fc9-4dab-87d0-1e6c5830144e"
      },
      "source": [
        "# installing gensim\n",
        "!pip install gensim==4.0.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gensim==4.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c3/dd/5e00b6e788a9c522b48f9df10472b2017102ffa65b10bc657471e0713542/gensim-4.0.0-cp37-cp37m-manylinux1_x86_64.whl (23.9MB)\n",
            "\u001b[K     |████████████████████████████████| 23.9MB 44.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim==4.0.0) (5.0.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim==4.0.0) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim==4.0.0) (1.19.5)\n",
            "Installing collected packages: gensim\n",
            "  Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed gensim-4.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jc7nwTB1OzKg",
        "outputId": "23a1ec4f-2ba4-48e1-a8b1-073476246e4c"
      },
      "source": [
        "!wget 'http://files.grouplens.org/datasets/movielens/ml-25m.zip' && unzip -o 'ml-25m.zip'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-17 14:30:16--  http://files.grouplens.org/datasets/movielens/ml-25m.zip\n",
            "Resolving files.grouplens.org (files.grouplens.org)... 128.101.65.152\n",
            "Connecting to files.grouplens.org (files.grouplens.org)|128.101.65.152|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 261978986 (250M) [application/zip]\n",
            "Saving to: ‘ml-25m.zip’\n",
            "\n",
            "ml-25m.zip          100%[===================>] 249.84M  16.8MB/s    in 16s     \n",
            "\n",
            "2021-05-17 14:30:33 (15.2 MB/s) - ‘ml-25m.zip’ saved [261978986/261978986]\n",
            "\n",
            "Archive:  ml-25m.zip\n",
            "   creating: ml-25m/\n",
            "  inflating: ml-25m/tags.csv         \n",
            "  inflating: ml-25m/links.csv        \n",
            "  inflating: ml-25m/README.txt       \n",
            "  inflating: ml-25m/ratings.csv      \n",
            "  inflating: ml-25m/genome-tags.csv  \n",
            "  inflating: ml-25m/genome-scores.csv  \n",
            "  inflating: ml-25m/movies.csv       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAEOo_JVOzKg",
        "outputId": "79feca14-c5e2-4cbd-9140-bbedfe32a2a1"
      },
      "source": [
        "# load and preprocess data\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "data_dir = Path.cwd() / 'ml-25m'\n",
        "movies_df = pd.read_csv(data_dir / 'movies.csv')\n",
        "tags_df = pd.read_csv(data_dir / 'tags.csv', converters={'tag': str}).groupby('movieId')['tag'].agg(list)\n",
        "df = pd.merge(movies_df[['movieId', 'title']], tags_df, how='right', left_on='movieId', right_index=True).rename(columns={'tag': 'tags'})\n",
        "df['movie_id'] = list(range(len(df)))\n",
        "\n",
        "# these dictionaries contain everything you need for the task\n",
        "movie_id_to_title = dict(zip(df['movie_id'], df['title']))  # maps movie_id to title\n",
        "title_to_movie_id = dict(zip(df['title'], df['movie_id']))  # maps title to movie_id\n",
        "movie_id_to_tags = dict(zip(df['movie_id'], df['tags']))    # maps movie_id to a list of tags. Use all the lists of tags as the input to train the word2vec model.\n",
        "\n",
        "print(f'number of movies in dataset: {len(movie_id_to_title)}')\n",
        "print(f'first movie: title: {movie_id_to_title[0]}')\n",
        "print(f'first movie: first 20 tags: {movie_id_to_tags[0][:20]}...')\n",
        "print(f'id of title \"Toy Story (1995)\": {title_to_movie_id[\"Toy Story (1995)\"]}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of movies in dataset: 45251\n",
            "first movie: title: Toy Story (1995)\n",
            "first movie: first 20 tags: ['Owned', 'imdb top 250', 'Pixar', 'Pixar', 'time travel', 'children', 'comedy', 'funny', 'witty', 'rated-G', 'animation', 'Pixar', 'computer animation', 'good cartoon chindren', 'pixar', 'friendship', 'bright', 'DARING RESCUES', 'fanciful', 'HEROIC MISSION']...\n",
            "id of title \"Toy Story (1995)\": 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLAxqueMOzKh"
      },
      "source": [
        "#### Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGdkAiIOOzKh"
      },
      "source": [
        "import torch\n",
        "from torch.nn.functional import cosine_similarity\n",
        "from gensim.models import Word2Vec # you can ignore any UserWarning about the missing levenshtein module\n",
        "\n",
        "# ToDo: Your solution goes here\n",
        "sentences = []\n",
        "for id in movie_id_to_tags:\n",
        "    sentences.append(movie_id_to_tags[id])\n",
        "\n",
        "model = Word2Vec(sentences, vector_size=64, window=5, min_count=1)\n",
        "model.save(\"word2vec.model\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "467R9k4f5Mrr"
      },
      "source": [
        "model = Word2Vec.load(\"word2vec.model\")\n",
        "word_vectors = model.wv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTx5TS3-Bgn6"
      },
      "source": [
        "def list_add(a,b):\n",
        "    c = []\n",
        "    for i in range(len(a)):\n",
        "        c.append(a[i]+b[i])\n",
        "    return c\n",
        "\n",
        "def list_div(v, d):\n",
        "    for i in range(len(v)):\n",
        "        v[i] = v[i] / d\n",
        "    return v "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_ZQWY-PKaPQ"
      },
      "source": [
        "movie_vectors = []\n",
        "for id in movie_id_to_tags:\n",
        "    tags = movie_id_to_tags[id]\n",
        "    vector = [0] * 64\n",
        "    T = len(tags)\n",
        "    for tag in tags:\n",
        "        vector = list_add(vector, word_vectors[tag])\n",
        "    vector = list_div(vector, T)\n",
        "    movie_vectors.append(vector)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7Zk1ZmGMLTt"
      },
      "source": [
        "title_to_vector = {}\n",
        "for i in range(len(movie_id_to_tags)):\n",
        "    title_to_vector[movie_id_to_title[i]] = movie_vectors[i]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wa7P4AFfPpeD"
      },
      "source": [
        "query = title_to_vector[\"Toy Story (1995)\"]\n",
        "score = {}\n",
        "for i in range(len(title_to_vector)):\n",
        "    score[movie_id_to_title[i]] = cosine_similarity(torch.Tensor(query), torch.Tensor(title_to_vector[movie_id_to_title[i]]), dim=-1)\n",
        "sorted_score = sorted(score.items(), key=lambda d:d[1], reverse=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HD77yEwfVCRY",
        "outputId": "4484d321-21e9-4e78-e282-39661ae8ff83"
      },
      "source": [
        "sorted_score[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Toy Story (1995)', tensor(1.)),\n",
              " ('Toy Story 2 (1999)', tensor(0.9921)),\n",
              " ('Finding Nemo (2003)', tensor(0.9914)),\n",
              " (\"Bug's Life, A (1998)\", tensor(0.9879)),\n",
              " ('Ice Age (2002)', tensor(0.9865)),\n",
              " ('Monsters, Inc. (2001)', tensor(0.9835)),\n",
              " ('Storks (2016)', tensor(0.9821)),\n",
              " ('The Secret Life of Pets (2016)', tensor(0.9786)),\n",
              " ('Ratatouille (2007)', tensor(0.9774)),\n",
              " (\"Emperor's New Groove, The (2000)\", tensor(0.9758))]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tl92kAaNVs9s"
      },
      "source": [
        "### C2. Sentiment Classification with LSTMs\n",
        "This task is about implementing a many-to-one LSTM for sentiment classification. We will use the IMDB dataset, which contains movie reviews associated with sentiments (positive/negative). The task is to classify each review into one of these two classes.\n",
        "\n",
        "We provide you with the following data setup:\n",
        "First, we install `torchtext` by ```pip install torchtext==0.9.1```. Then we load the IMDB dataset from `torchtext.datasets`, build the vocabulary and split to train/valid/test instances using predefined functions from `torchtext.data`. Don't worry if you get confused by the dataset API, lets focus on the model architecture and training methods."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXjM5L4sOzKi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfdcdc58-03a4-47dc-95c8-e4ef880f22ce"
      },
      "source": [
        "!pip install torchtext==0.9.1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchtext==0.9.1 in /usr/local/lib/python3.7/dist-packages (0.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.1) (1.19.5)\n",
            "Requirement already satisfied: torch==1.8.1 in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.1) (1.8.1+cu101)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.1) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.1) (4.41.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1->torchtext==0.9.1) (3.7.4.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.1) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.1) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.1) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.1) (3.0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-IlyXicOzKi"
      },
      "source": [
        "import re\n",
        "import torch\n",
        "from torchtext.legacy import data, datasets\n",
        "\n",
        "max_length = 200   # we want the maximum words in each text instance to be 200.\n",
        "max_vocab = 20000  # We want the vocabulary size not to exceed 20000.\n",
        "\n",
        "# define a function to preprocess raw text input, define the input and output field.\n",
        "text_cleaner = lambda X: [re.sub(r'[^\\w\\s]', '', x) for x in X]  \n",
        "TEXT = data.Field(preprocessing=text_cleaner, lower=True, batch_first=True, fix_length=max_length)\n",
        "LABEL = data.LabelField()\n",
        "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)  # This will download IMDB for the first run.\n",
        "train_data, valid_data = train_data.split(split_ratio=0.7)\n",
        "\n",
        "# Init the vocabulary from training data. \n",
        "TEXT.build_vocab(train_data, max_size= max_vocab)\n",
        "LABEL.build_vocab(train_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgXYLMWbOzKj"
      },
      "source": [
        "You can get the text vocabulary size with `len(TEXT.vocab)`. Note that two special tokens `('<pad>', '<unk>')` are added to the vocabulary. You can check the index by e.g, `TEXT.vocab.stoi['<pad>']` and the inverse index by `TEXT.vocab.itos[1]`. Similarly, to check the index for labels, you can use `LABEL.vocab`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p53YEJPTOzKj"
      },
      "source": [
        "Next, we make the dataset iteratable (in batches) for training the model. We need to decide on the batch size, and already send the data to the gpu or cpu device. Note that each batch of data contains a tuple of input text and output label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXuOaZRSOzKj"
      },
      "source": [
        "batch_size = 32   # you can change this size.\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "train_iter, valid_iter, test_iter = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "    batch_size = batch_size,\n",
        "    sort_within_batch = True,\n",
        "    device = device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEc-OMh2OzKk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9536d1df-1c9b-470c-800d-6679549dce9a"
      },
      "source": [
        "# example of iterating the training data\n",
        "for batch in train_iter:\n",
        "    print(batch.text) # tensor of size (batch_size x max_length) containing the tokenized words\n",
        "    print(batch.label) # 1d tensor containing the binary labels\n",
        "    print(f'first sentence in batch:\\n{\" \".join([TEXT.vocab.itos[token_id] for token_id in batch.text[0]])}')\n",
        "    print(f'first label in batch: {LABEL.vocab.itos[batch.label[0]]}')\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[132,  10,  60,  ...,   1,   1,   1],\n",
            "        [250, 528,  39,  ...,   1,   1,   1],\n",
            "        [150,  10, 175,  ...,   1,   1,   1],\n",
            "        ...,\n",
            "        [ 11,  20,   7,  ...,   1,   1,   1],\n",
            "        [ 58, 601,  14,  ...,   1,   1,   1],\n",
            "        [ 10, 417, 386,  ...,   1,   1,   1]])\n",
            "tensor([1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
            "        0, 1, 1, 1, 0, 1, 0, 1])\n",
            "first sentence in batch:\n",
            "while i would say i enjoy the show i expected something completely different from when i first saw what i like about you i expected to find something along the lines of all that i am not sure if it is going on anymore but i have to say i do like the show and while i dont classify it as a breakthrough show it is very charming and i do like the chemistry between the characters as well including the supporting castbr br i would definitely say that it is great to see wesley jonathan back on the screen because i really loved him in city guy i had also seen the woman who plays <unk> friend in popular and while i think that was an okay show i do not really like her character in this show because shes just not my cup of tea but she rounds it out pretty well <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "first label in batch: pos\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwXq5cIdOzKk"
      },
      "source": [
        "Now, the datasets have been prepared.\n",
        "\n",
        "\n",
        "***Your task is to*** : \n",
        "\n",
        "1. Define a model named SentimentClassifier with following features:\n",
        "* An Embedding layer with 100 embedding dimension. Use `nn.Embedding`.\n",
        "* One bidirectional LSTM layer with hidden dimension 400. Use `nn.LSTM` and set `bidirectional=True`.\n",
        "* Some dropout with probability 0.3 on the LSTM output. Use `nn.Dropout`.\n",
        "* One fully connected layer to map the output features of the LSTM layer to a single output. Use a sigmoid activation for the output.\n",
        "\n",
        "2. Train your model with binary cross entropy loss (`torch.nn.BCELoss`) and the adam optimizer (use `torch.optim.Adam` with default parameters) for a maximum of 20 epochs. After every epoch, compute the validation loss. Implement an early stopping mechanism that keeps track of the best model parameters based on lowest validation loss. Stop training if the validation loss does not improve for continuous 3 epochs and revert back to the best model.\n",
        "\n",
        "3. Evaluate the accurracy of your trained model on the `test_iter` dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDGlDwRYOzKl"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# todo: fill the __init__() and forward() function.\n",
        "class SentimentClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SentimentClassifier, self).__init__()\n",
        "\n",
        "        self.output_size = 1\n",
        "        self.n_layers = 2\n",
        "        self.hidden_dim = 400\n",
        "        self.vocab_size = 20002\n",
        "        self.embedding_dim = 100\n",
        "        self.bidirectional = True\n",
        "        self.text_size=200\n",
        "\n",
        "\n",
        "        self.embed = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
        "        self.lstm = nn.LSTM(self.embedding_dim, self.hidden_dim, self.n_layers,\n",
        "                            bidirectional=self.bidirectional)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.fc = nn.Linear(800, 1)\n",
        "        self.sig = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, text, hidden):\n",
        "        batch_size = text.shape[0]\n",
        "        embed_out = self.embed(text)\n",
        "        lstm_out, hidden = self.lstm(embed_out, hidden)\n",
        "        out = lstm_out.contiguous().view(-1, self.hidden_dim*2)\n",
        "        out = self.dropout(lstm_out)\n",
        "        out = self.fc(out)\n",
        "        sig_out = self.sig(out)\n",
        "\n",
        "        sig_out = sig_out.view(batch_size, -1)\n",
        "        sig_out = sig_out[:, -1]\n",
        "        return sig_out, hidden\n",
        "    \n",
        "    def init_hidden(self):\n",
        "        weight = next(self.parameters()).data\n",
        "        hidden = (weight.new(self.n_layers*2, self.text_size, self.hidden_dim).zero_(),\n",
        "                  weight.new(self.n_layers*2, self.text_size, self.hidden_dim).zero_())\n",
        "        return hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGmQSwPwOzKl"
      },
      "source": [
        "Defining `loss function` and `optimizer`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZBto_D2OzKl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e96a019-4b89-4030-fc85-288ef23c097b"
      },
      "source": [
        "# todo\n",
        "net = SentimentClassifier()\n",
        "print(net)\n",
        "\n",
        "loss_function = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SentimentClassifier(\n",
            "  (embed): Embedding(20002, 100)\n",
            "  (lstm): LSTM(100, 400, num_layers=2, bidirectional=True)\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (fc): Linear(in_features=800, out_features=1, bias=True)\n",
            "  (sig): Sigmoid()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6oojI1xbZ8hO",
        "outputId": "37184115-d601-4e90-c4f9-1e046848e65f"
      },
      "source": [
        "train_on_gpu=torch.cuda.is_available()\n",
        " \n",
        "if(train_on_gpu):\n",
        "    print('Training on GPU.')\n",
        "else:\n",
        "    print('No GPU available, training on CPU.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No GPU available, training on CPU.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lEI2gWtOzKl"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vomxSP-PymGl"
      },
      "source": [
        "please retrain this cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjAmKojzOzKl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        },
        "outputId": "031efc22-da80-4b8c-a73b-ac03b16b7d81"
      },
      "source": [
        "# todo\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "Max_epoch = 20\n",
        "clip = 5\n",
        "\n",
        "if(train_on_gpu):\n",
        "    net.cuda()\n",
        "\n",
        "net.train()\n",
        "total_val_loss = []\n",
        "best_val_loss = 1000\n",
        "for e in range(Max_epoch):\n",
        "    \n",
        "    h = net.init_hidden()\n",
        "    counter = 0\n",
        " \n",
        "    for text, labels in train_iter:\n",
        "        counter += 1\n",
        " \n",
        "        if(train_on_gpu):\n",
        "            text, labels = text.cuda(), labels.cuda()\n",
        "\n",
        "        h = tuple([each.data for each in h])\n",
        "        net.zero_grad()\n",
        "\n",
        "        output, h = net(text, h)\n",
        "\n",
        "        loss = loss_function(output.squeeze(), labels.float())\n",
        "        loss.backward()\n",
        "\n",
        "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "        optimizer.step()\n",
        "        #print(loss)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    val_h = net.init_hidden()\n",
        "    val_losses = []\n",
        "    net.eval()\n",
        "    for text, labels in valid_iter:\n",
        " \n",
        "        val_h = tuple([each.data for each in val_h])\n",
        " \n",
        "        if(train_on_gpu):\n",
        "            text, labels = text.cuda(), labels.cuda()\n",
        " \n",
        "        output, val_h = net(text, val_h)\n",
        "        val_loss = loss_function(output.squeeze(), labels.float())\n",
        " \n",
        "        val_losses.append(val_loss.item())\n",
        "    net.train()\n",
        "\n",
        "\n",
        "    total_val_loss.append(np.mean(val_losses))\n",
        "    \n",
        "    print(\"Epoch: {}/{}...\".format(e+1, Max_epoch),\n",
        "          \"Step: {}...\".format(counter),\n",
        "          \"Val Loss: {:.4f}\".format(total_val_loss[e]))\n",
        "\n",
        "    if e == 0:\n",
        "        torch.save(net, \"best_net.pth\")\n",
        "        best_val_loss = total_val_loss[e]\n",
        "        print(\"first save: epoch 1\")\n",
        "    if e>=1 and total_val_loss[e] <= best_val_loss:\n",
        "        torch.save(net, \"best_net.pth\")\n",
        "        best_val_loss = total_val_loss[e]\n",
        "        print(\"last save: epoch\", e+1)\n",
        "    \n",
        "    if e>2 and total_val_loss[e] > best_val_loss and total_val_loss[e-1] > best_val_loss and total_val_loss[e-2] > best_val_loss:\n",
        "        print(\"Early Stopping\\n\")\n",
        "        print(\"best val loss: {:.4f}\".format(best_val_loss))\n",
        "        break\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-59cf5aa3b0c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# calculate the loss and perform backprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0;31m# `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m#nn.utils.clip_grad_norm_(net.parameters(), clip)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jc1GhFkfOzKm"
      },
      "source": [
        "Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mP-8QUKNOzKm"
      },
      "source": [
        "# todo\n",
        "net = torch.load(\"best_net.pth\")\n",
        "test_h = net.init_hidden()\n",
        "test_losses = []\n",
        "accuracy = []\n",
        "for text, labels in test_iter:\n",
        " \n",
        "    if(train_on_gpu):\n",
        "        text, labels = text.cuda(), labels.cuda()\n",
        "        \n",
        "    test_h = tuple([each.data for each in test_h])\n",
        "    # zero accumulated gradients\n",
        "    net.zero_grad()\n",
        " \n",
        "    \n",
        "    output, test_h = net(text, test_h)\n",
        "    test_loss = loss_function(output.squeeze(), labels.float())\n",
        "    test_losses.append(test_loss.item())\n",
        "    \n",
        "    correct = 0\n",
        "    for i in range(len(output)):\n",
        "        if output[i] >= 0.5:\n",
        "            output[i] = 1\n",
        "        else:\n",
        "            output[i] = 0\n",
        "        if output[i] == labels[i]:\n",
        "            correct += 1\n",
        "    accuracy.append(correct / len(output))\n",
        "    \n",
        "print(\"total test loss : %.4f\" % np.mean(test_losses))\n",
        "print(\"total test accuracy: %4f\" % np.mean(accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}